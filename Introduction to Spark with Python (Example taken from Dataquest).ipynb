{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark with Python\n",
    "### This example is from the [Dataquest](https://www.dataquest.io/mission/123/introduction-to-spark/4/spark-context) lesson.\n",
    "This notebook is a basic introductory demonstration of how to interact with Resilient Distributed Datasets (RDD's), a core data structure in Spark, with Python. \n",
    "Thanks to PySpark, we can interface with RDD's using Python instead of Scala, the language Spark is written in. \n",
    "In this notebook, we are using a dataset that conatains all of the guests of the Daily Show.\n",
    "\n",
    "The records are labeled by:\n",
    "<ul>\n",
    "<li>Year</li>\n",
    "<li>Occupation</li>\n",
    "<li>Show (data)</li>\n",
    "<li>Group</li>\n",
    "<li>Guest List</li>\n",
    "</ul>\n",
    "           \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing successful connection to spark cluster from notebook\n",
    "Spark Context (sc) is the object that manages the connection to the clusters in Spark and coordinates running processes on the clusters themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x1048066d8>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "   The key to understanding Spark is the idea of data pipelining. Every operation in Spark is a series of steps chained together and run in succession to form a pipeline. Each step in the pipeline returns either a Python value, a Python data structure, or an RDD object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in csv file of daily show guests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = sc.textFile (\"./data/daily_show_guests.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR,GoogleKnowlege_Occupation,Show,Group,Raw_Guest_List',\n",
       " '1999,actor,1/11/99,Acting,Michael J. Fox',\n",
       " '1999,Comedian,1/12/99,Comedy,Sandra Bernhard',\n",
       " '1999,television actress,1/13/99,Acting,Tracey Ullman',\n",
       " '1999,film actress,1/14/99,Acting,Gillian Anderson']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the RDD  by the ',' deliminator\n",
    "Sice RDD's are immutable, they must be assigned to a new RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['YEAR', 'GoogleKnowlege_Occupation', 'Show', 'Group', 'Raw_Guest_List'],\n",
       " ['1999', 'actor', '1/11/99', 'Acting', 'Michael J. Fox'],\n",
       " ['1999', 'Comedian', '1/12/99', 'Comedy', 'Sandra Bernhard'],\n",
       " ['1999', 'television actress', '1/13/99', 'Acting', 'Tracey Ullman'],\n",
       " ['1999', 'film actress', '1/14/99', 'Acting', 'Gillian Anderson']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_show = raw_data.map(lambda line: line.split(','))\n",
    "daily_show.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice tally does not print because of Spark's \"Lazy Operation\"\n",
    "Because of Lazy Operation, PySpark delays execution of tranformation operations until we actually need it. (e.g take( ) to preview the first few elements in tally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[21] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "tally = daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)\n",
    "print(tally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 1),\n",
       " ('2012', 164),\n",
       " ('2013', 166),\n",
       " ('2004', 164),\n",
       " ('2011', 163),\n",
       " ('2014', 163),\n",
       " ('2002', 159),\n",
       " ('2007', 141),\n",
       " ('2015', 100),\n",
       " ('2003', 166),\n",
       " ('2010', 165),\n",
       " ('2001', 157),\n",
       " ('2000', 169),\n",
       " ('2008', 164),\n",
       " ('2005', 162),\n",
       " ('2009', 163),\n",
       " ('1999', 166),\n",
       " ('2006', 161)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tally.take(tally.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlike Pandas, PySpark does not remove the header row. So, we will need to do this ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_year(line):\n",
    "    if line[0] == 'YEAR':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "filtered_daily_show = daily_show.filter(lambda line:filter_year(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All together now\n",
    "To flex Spark's muscles, we'll demonstrate how to chain together a series of data transformations into a pipeline and observe Spark managing everything in the background. Previously, running lots of tasks in succession in Hadoop was very time consuming since intermediate results needed to be written to diskd and Hadoop wasn't aware of the full pipeline. \n",
    "\n",
    "Thanks to Spark's aggressive usage of memory(and only disk as a backup and for specific tasks) and well architected core, Spark is able to improve significantly on Hadoop's turnaround time. \n",
    "\n",
    "In the following code, we'll filter out actors with no profession listed, lowercase each profession, generate a histogram of professions, and output the first 5 tuples in the histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('radio personality', 3),\n",
       " ('former governor of new york', 1),\n",
       " ('illustrator', 1),\n",
       " ('presidnet', 3),\n",
       " ('former united states secretary of state', 6)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_daily_show.filter(lambda line: line[1] != '') \\\n",
    "                   .map(lambda line: (line[1].lower(), 1))\\\n",
    "                   .reduceByKey(lambda x,y: x+y) \\\n",
    "                   .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All copyrights belong to their respective owners including Dataquest.\n",
    "\n",
    "Code and text are used here only for education, and are not intended to generate income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
